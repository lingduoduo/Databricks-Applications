{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 1: Basic Prompt Structure\n",
    "\n",
    "- [Lesson](#lesson)\n",
    "- [Exercises](#exercises)\n",
    "- [Example Playground](#example-playground)\n",
    "\n",
    "## Setup\n",
    "\n",
    "Run the following setup cell to load your API key and establish the `get_completion` helper function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "astropy 6.1.0 requires numpy>=1.23, but you have numpy 1.22.4 which is incompatible.\n",
      "scikit-image 0.24.0 requires numpy>=1.23, but you have numpy 1.22.4 which is incompatible.\n",
      "sparkmagic 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.2.2 which is incompatible.\n",
      "sphinx 8.0.2 requires docutils<0.22,>=0.20, but you have docutils 0.16 which is incompatible.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip --quiet --no-color\n",
    "%pip install anthropic --force-reinstall --quiet --no-color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import python's built-in regular expression library\n",
    "import re\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import json\n",
    "\n",
    "# Import the hints module from the utils package\n",
    "from utils import hints\n",
    "\n",
    "# Retrieve the MODEL_NAME variable from the IPython store\n",
    "%store -r modelId\n",
    "%store -r region\n",
    "\n",
    "bedrock_client = boto3.client(service_name='bedrock-runtime', region_name=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'anthropic.claude-3-haiku-20240307-v1:0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, system_prompt=None):\n",
    "    # Define the inference configuration\n",
    "    inference_config = {\n",
    "        \"temperature\": 0.0,  # Set the temperature for generating diverse responses\n",
    "        \"maxTokens\": 200,  # Set the maximum number of tokens to generate\n",
    "        \"topP\": 1,  # Set the top_p value for nucleus sampling\n",
    "    }\n",
    "    # Create the converse method parameters\n",
    "    converse_api_params = {\n",
    "        \"modelId\": modelId,  # Specify the model ID to use\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": [{\"text\": prompt}]}],  # Provide the user's prompt\n",
    "        \"inferenceConfig\": inference_config,  # Pass the inference configuration\n",
    "    }\n",
    "    # Check if system_text is provided\n",
    "    if system_prompt:\n",
    "        # If system_text is provided, add the system parameter to the converse_params dictionary\n",
    "        converse_api_params[\"system\"] = [{\"text\": system_prompt}]\n",
    "\n",
    "    # Send a request to the Bedrock client to generate a response\n",
    "    try:\n",
    "        response = bedrock_client.converse(**converse_api_params)\n",
    "\n",
    "        # Extract the generated text content from the response\n",
    "        text_content = response['output']['message']['content'][0]['text']\n",
    "\n",
    "        # Return the generated text content\n",
    "        return text_content\n",
    "\n",
    "    except ClientError as err:\n",
    "        message = err.response['Error']['Message']\n",
    "        print(f\"A client error occured: {message}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Lesson\n",
    "\n",
    "Amazon Bedrock offers three APIs that can be used with Anthropic Claude models, the legacy [Text Completions API](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-anthropic-claude-text-completion.html) , the [Messages API](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-anthropic-claude-messages.html) and the current [Converse API](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html). For this tutorial, we will be exclusively using the Converse API.\n",
    "\n",
    "At minimum, a call to Claude using the Converse API requires the following parameters:\n",
    "- `modelId`: the [API model name](https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html#model-ids-arns) of the model that you intend to call\n",
    "\n",
    "- `messages`: an array of input messages. Claude 3 models are trained to operate on alternating `user` and `assistant` conversational turns. When creating a new `Message`, you specify the prior conversational turns with the messages parameter, and the model then generates the next `Message` in the conversation.\n",
    "  - Each input message must be an object with a `role` and `content`. You can specify a single `user`-role message, or you can include multiple `user` and `assistant` messages (they must alternate, if so). **The first message must always use the `user` role.**\n",
    "  \n",
    "  You store the content for the message in the `content` field of a [(ContentBlock)](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_ContentBlock.html). Specify text in the `text` field, or if supported by the model, you can also pass the raw bytes for an image in the `image` field of an [(ImageBlock)](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_ImageBlock.html). The other fields in ContentBlock are for [tool use](https://docs.aws.amazon.com/bedrock/latest/userguide/tool-use.html).\n",
    "\n",
    "There are also optional parameters, such as:\n",
    "- `system`: the system prompt - more on this below.\n",
    "  \n",
    "- `temperature`: the degree of variability in Claude's response. For these lessons and exercises, we have set `temperature` to 0.\n",
    "\n",
    "- `max_tokens`: the maximum number of tokens to generate before stopping. Note that Claude may stop before reaching this maximum. This parameter only specifies the absolute maximum number of tokens to generate. Furthermore, this is a *hard* stop, meaning that it may cause Claude to stop generating mid-word or mid-sentence.\n",
    "\n",
    "For a complete list of all API parameters, visit our [API documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples\n",
    "\n",
    "Let's take a look at how Claude responds to some correctly-formatted prompts. For each of the following cells, run the cell (`shift+enter`), and Claude's response will appear below the block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm doing well, thanks for asking! As an AI assistant, I don't have feelings in the same way humans do, but I'm functioning properly and ready to assist you with any questions or tasks you may have. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "# Prompt\n",
    "PROMPT = \"Hi Claude, how are you?\"\n",
    "\n",
    "# Print Claude's response\n",
    "print(get_completion(PROMPT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The color of the ocean can vary depending on a few factors:\n",
      "\n",
      "- Depth - The ocean appears blue in deeper waters due to the way sunlight interacts with the water molecules. Shallower waters may appear more green, turquoise, or even brown.\n",
      "\n",
      "- Suspended particles - Things like plankton, sediment, or algae in the water can affect the ocean's color, making it appear more green, brown, or even reddish.\n",
      "\n",
      "- Weather conditions - The sky's color can reflect onto the surface of the ocean, making it appear more gray, white, or even darker blue on overcast days.\n",
      "\n",
      "- Geographic location - Different regions of the ocean can have slightly different hues based on factors like latitude, currents, and local marine life.\n",
      "\n",
      "In general though, the predominant color of the open ocean is a deep, rich blue. This is due to the way sunlight interacts with the water and the lack of\n"
     ]
    }
   ],
   "source": [
    "# Prompt\n",
    "PROMPT = \"Can you tell me the color of the ocean?\"\n",
    "\n",
    "# Print Claude's response\n",
    "print(get_completion(PROMPT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Celine Dion was born on March 30, 1968.\n"
     ]
    }
   ],
   "source": [
    "# Prompt\n",
    "PROMPT = \"What year was Celine Dion born in?\"\n",
    "\n",
    "# Print Claude's response\n",
    "print(get_completion(PROMPT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at some prompts that do not include the correct Converse API formatting. For these malformatted prompts, the Converse API returns an error.\n",
    "\n",
    "First, we have an example of a Converse API call that lacks `role` and `content` fields in the `messages` array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ⚠️ **Warning:** Due to the incorrect formatting of the messages parameter in the prompt, the following cell will return an error. This is expected behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: Parameter validation failed:\n",
      "Invalid type for parameter messages[0].content[0], value: Hi Claude, how are you?, type: <class 'str'>, valid types: <class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "# Ensure modelId is defined somewhere in your code\n",
    "modelId = 'anthropic.claude-3-haiku-20240307-v1:0'  # Replace with your actual model ID\n",
    "\n",
    "# Configuration for inference\n",
    "inference_config = {\n",
    "    \"temperature\": 0.0,  # Set the temperature for generating diverse responses\n",
    "    \"maxTokens\": 200     # Set the maximum number of tokens to generate\n",
    "}\n",
    "\n",
    "# Parameters for the converse API call\n",
    "converse_api_params = {\n",
    "    \"modelId\": modelId,\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": [\"Hi Claude, how are you?\"]}  # Content should be a list\n",
    "    ],\n",
    "    \"inferenceConfig\": inference_config  # Pass the inference configuration\n",
    "}\n",
    "\n",
    "# Making the API call\n",
    "try:\n",
    "    response = bedrock_client.converse(**converse_api_params)\n",
    "    \n",
    "    # Assuming the response structure has 'output' -> 'messages' -> list of messages -> 'content'\n",
    "    # Adjust according to your actual response format\n",
    "    if 'output' in response and 'messages' in response['output']:\n",
    "        # Extracting the content from the first message in the response\n",
    "        message_content = response['output']['messages'][0]['content']\n",
    "        # Join the content list into a single string if necessary\n",
    "        print(\" \".join(message_content))\n",
    "    else:\n",
    "        print(\"Unexpected response format:\", response)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a prompt that fails to alternate between the `user` and `assistant` roles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ⚠️ **Warning:** Due to the lack of alternation between `user` and `assistant` roles, Claude will return an error message. This is expected behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValidationException",
     "evalue": "An error occurred (ValidationException) when calling the Converse operation: A conversation must alternate between user and assistant roles. Make sure the conversation alternates between user and assistant roles and try again.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationException\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 15\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Create the converse method parameters\u001b[39;00m\n\u001b[1;32m      6\u001b[0m converse_api_params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodelId\u001b[39m\u001b[38;5;124m\"\u001b[39m: modelId,\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minferenceConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m: inference_config,\n\u001b[1;32m     13\u001b[0m }\n\u001b[0;32m---> 15\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mbedrock_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverse\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconverse_api_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Print Claude's response\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/botocore/client.py:569\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    566\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    567\u001b[0m     )\n\u001b[1;32m    568\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 569\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/botocore/client.py:1023\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1019\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m error_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryErrorCode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m error_info\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   1020\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1021\u001b[0m     )\n\u001b[1;32m   1022\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m-> 1023\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1025\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mValidationException\u001b[0m: An error occurred (ValidationException) when calling the Converse operation: A conversation must alternate between user and assistant roles. Make sure the conversation alternates between user and assistant roles and try again."
     ]
    }
   ],
   "source": [
    "inference_config = {\n",
    "    \"temperature\": 0.5,\n",
    "    \"maxTokens\": 200\n",
    "}\n",
    "# Create the converse method parameters\n",
    "converse_api_params = {\n",
    "    \"modelId\": modelId,\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": [{\"text\": \"What year was Celine Dion born in?\"}]},\n",
    "        {\"role\": \"user\", \"content\": [{\"text\":\"Also, can you tell me some other facts about her?\"}]}\n",
    "    ],\n",
    "    \"inferenceConfig\": inference_config,\n",
    "}\n",
    "\n",
    "response = bedrock_client.converse(**converse_api_params)\n",
    "\n",
    "# Print Claude's response\n",
    "print(response['output']['message']['content'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`user` and `assistant` messages **MUST alternate**, and messages **MUST start with a `user` turn**. You can have multiple `user` & `assistant` pairs in a prompt (as if simulating a multi-turn conversation). You can also put words into a terminal `assistant` message for Claude to continue from where you left off (more on that in later chapters).\n",
    "\n",
    "#### System Prompts\n",
    "\n",
    "You can also use **system prompts**. A system prompt is a way to **provide context, instructions, and guidelines to Claude** before presenting it with a question or task in the \"User\" turn. \n",
    "\n",
    "Structurally, system prompts exist separately from the list of `user` & `assistant` messages, and thus belong in a separate `system` parameter (take a look at the structure of the `get_completion` helper function in the [Setup](#setup) section of the notebook). \n",
    "\n",
    "Within this tutorial, wherever we might utilize a system prompt, we have provided you a `system` field in your completions function. Should you not want to use a system prompt, simply set the `SYSTEM_PROMPT` variable to an empty string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### System Prompt Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_completion' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m PROMPT \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhy is the sky blue?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Print Claude's response\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mget_completion\u001b[49m(PROMPT, SYSTEM_PROMPT))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_completion' is not defined"
     ]
    }
   ],
   "source": [
    "# System prompt\n",
    "SYSTEM_PROMPT = \"Your answer should always be a series of critical thinking questions that further the conversation (do not provide answers to your questions). Do not actually answer the user question.\"\n",
    "\n",
    "# Prompt\n",
    "PROMPT = \"Why is the sky blue?\"\n",
    "\n",
    "# Print Claude's response\n",
    "print(get_completion(PROMPT, SYSTEM_PROMPT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why use a system prompt? A **well-written system prompt can improve Claude's performance** in a variety of ways, such as increasing Claude's ability to follow rules and instructions. For more information, visit Anthropic's documentation on [how to use system prompts](https://docs.anthropic.com/claude/docs/how-to-use-system-prompts) with Claude.\n",
    "\n",
    "Now we'll dive into some exercises. If you would like to experiment with the lesson prompts without changing any content above, scroll all the way to the bottom of the lesson notebook to visit the [**Example Playground**](#example-playground)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises\n",
    "- [Exercise 1.1 - Counting to Three](#exercise-11---counting-to-three)\n",
    "- [Exercise 1.2 - System Prompt](#exercise-12---system-prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1 - Counting to Three\n",
    "Using proper `user` / `assistant` formatting, edit the `PROMPT` below to get Claude to **count to three.** The output will also indicate whether your solution is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, 2, 3.\n",
      "\n",
      "--------------------------- GRADING ---------------------------\n",
      "This exercise has been correctly solved: True\n"
     ]
    }
   ],
   "source": [
    "# Prompt - this is the only field you should change\n",
    "# PROMPT = \"[Replace this text]\"\n",
    "PROMPT = \"[Count from one to three]\"\n",
    "\n",
    "# Get Claude's response\n",
    "response = get_completion(PROMPT)\n",
    "\n",
    "# Function to grade exercise correctness\n",
    "def grade_exercise(text):\n",
    "    pattern = re.compile(r'^(?=.*1)(?=.*2)(?=.*3).*$', re.DOTALL)\n",
    "    return bool(pattern.match(text))\n",
    "\n",
    "# Print Claude's response and the corresponding grade\n",
    "print(response)\n",
    "print(\"\\n--------------------------- GRADING ---------------------------\")\n",
    "print(\"This exercise has been correctly solved:\", grade_exercise(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ If you want a hint, run the cell below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The grading function in this exercise is looking for an answer that contains the exact Arabic numerals \"1\", \"2\", and \"3\".\n",
      "You can often get Claude to do what you want simply by asking.\n"
     ]
    }
   ],
   "source": [
    "print(hints.exercise_1_1_hint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.2 - System Prompt\n",
    "\n",
    "Modify the `SYSTEM_PROMPT` to make Claude respond like it's a 3 year old child."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sky is very large, but it's difficult to give an exact size for it. Here are some key points about the size of the sky:\n",
      "\n",
      "- The sky refers to the area of space above the Earth's surface. It extends as far as the Earth's atmosphere goes, which is about 6,200 miles (10,000 km) above the surface.\n",
      "\n",
      "- Beyond the Earth's atmosphere, the sky transitions into outer space, which is essentially limitless. The observable universe is estimated to be about 93 billion light-years in diameter.\n",
      "\n",
      "- The visible portion of the sky that we can see from Earth's surface is called the celestial sphere. This appears to be a dome or hemisphere covering the Earth, but it's actually just our perspective of a much larger expanse.\n",
      "\n",
      "- The size of the visible sky depends on your location on Earth. From a single vantage point, the visible sky covers about 180-200 degrees horizont\n",
      "\n",
      "--------------------------- GRADING ---------------------------\n",
      "This exercise has been correctly solved: False\n"
     ]
    }
   ],
   "source": [
    "# System prompt - this is the only field you should change\n",
    "SYSTEM_PROMPT = \"a 3 year old child assistant\"\n",
    "\n",
    "# Prompt\n",
    "PROMPT = \"How big is the sky?\"\n",
    "\n",
    "# Get Claude's response\n",
    "response = get_completion(PROMPT, SYSTEM_PROMPT)\n",
    "\n",
    "# Function to grade exercise correctness\n",
    "def grade_exercise(text):\n",
    "    return bool(re.search(r\"giggles\", text) or re.search(r\"soo\", text))\n",
    "\n",
    "# Print Claude's response and the corresponding grade\n",
    "print(response)\n",
    "print(\"\\n--------------------------- GRADING ---------------------------\")\n",
    "print(\"This exercise has been correctly solved:\", grade_exercise(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ If you want a hint, run the cell below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The grading function in this exercise is looking for answers that contain \"soo\" or \"giggles\".\n",
      "There are many ways to solve this, just by asking!\n"
     ]
    }
   ],
   "source": [
    "print(hints.exercise_1_2_hint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congrats!\n",
    "\n",
    "If you've solved all exercises up until this point, you're ready to move to the next chapter. Happy prompting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Example Playground\n",
    "\n",
    "This is an area for you to experiment freely with the prompt examples shown in this lesson and tweak prompts to see how it may affect Claude's responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm doing well, thanks for asking! As an AI assistant, I don't have feelings in the same way humans do, but I'm functioning properly and ready to assist you with any questions or tasks you may have. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "# Prompt\n",
    "PROMPT = \"Hi Claude, how are you?\"\n",
    "\n",
    "# Print Claude's response\n",
    "print(get_completion(PROMPT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The color of the ocean can vary depending on a few factors:\n",
      "\n",
      "- Depth - The ocean appears blue in deeper waters due to the way sunlight interacts with the water molecules. Shallower waters may appear more green, turquoise, or even brown.\n",
      "\n",
      "- Suspended particles - Things like plankton, sediment, or algae in the water can affect the ocean's color, making it appear more green, brown, or even reddish.\n",
      "\n",
      "- Weather conditions - The sky's color can reflect onto the surface of the ocean, making it appear more gray, white, or even darker blue on overcast days.\n",
      "\n",
      "- Geographic location - Different regions of the ocean can have slightly different hues based on factors like latitude, currents, and local marine life.\n",
      "\n",
      "In general though, the predominant color of the open ocean is a deep, rich blue. This is due to the way sunlight interacts with the water and the lack of\n"
     ]
    }
   ],
   "source": [
    "# Prompt\n",
    "PROMPT = \"Can you tell me the color of the ocean?\"\n",
    "\n",
    "# Print Claude's response\n",
    "print(get_completion(PROMPT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Celine Dion was born on March 30, 1968.\n"
     ]
    }
   ],
   "source": [
    "# Prompt\n",
    "PROMPT = \"What year was Celine Dion born in?\"\n",
    "\n",
    "# Print Claude's response\n",
    "print(get_completion(PROMPT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: Parameter validation failed:\n",
      "Invalid type for parameter messages[0].content, value: Hi Claude, how are you?, type: <class 'str'>, valid types: <class 'list'>, <class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "# Ensure modelId is defined somewhere in your code\n",
    "modelId = 'anthropic.claude-3-haiku-20240307-v1:0' # Replace with your actual model ID\n",
    "\n",
    "# Configuration for inference\n",
    "inference_config = {\n",
    "    \"temperature\": 0.0  # Control the randomness of responses\n",
    "}\n",
    "\n",
    "# Additional fields for model request\n",
    "additional_model_fields = {\n",
    "    \"max_tokens\": 200  # Set the maximum number of tokens for the response\n",
    "}\n",
    "\n",
    "# Parameters for the converse API call\n",
    "converse_api_params = {\n",
    "    \"modelId\": modelId,\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"Hi Claude, how are you?\"}  # Correct format for messages\n",
    "    ],\n",
    "    \"inferenceConfig\": inference_config,\n",
    "    \"additionalModelRequestFields\": additional_model_fields\n",
    "}\n",
    "\n",
    "# Making the API call\n",
    "try:\n",
    "    response = bedrock_client.converse(**converse_api_params)\n",
    "    \n",
    "    # Adjust the response parsing based on the actual API response structure\n",
    "    # This assumes response['output']['messages'][0]['content'] contains the text\n",
    "    if 'output' in response and 'messages' in response['output']:\n",
    "        message_content = response['output']['messages'][0]['content']\n",
    "        print(message_content)  # Directly print the content assuming it's a string\n",
    "    else:\n",
    "        print(\"Unexpected response format:\", response)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: name 'SomeBedrockClientLibrary' is not defined\n"
     ]
    }
   ],
   "source": [
    "# Ensure modelId is defined somewhere in your code\n",
    "modelId = 'anthropic.claude-3-haiku-20240307-v1:0'  # Replace with your actual model ID\n",
    "\n",
    "# Configuration for inference\n",
    "inference_config = {\n",
    "    \"temperature\": 0.0\n",
    "}\n",
    "\n",
    "# Additional fields for model request\n",
    "additional_model_fields = {\n",
    "    \"top_p\": 1,\n",
    "    \"max_tokens\": 200\n",
    "}\n",
    "\n",
    "# Parameters for the converse API call\n",
    "converse_api_params = {\n",
    "    \"modelId\": modelId,\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"What year was Celine Dion born in?\"},\n",
    "        {\"role\": \"user\", \"content\": \"Also, can you tell me some other facts about her?\"}\n",
    "    ],\n",
    "    \"inferenceConfig\": inference_config,\n",
    "    \"additionalModelRequestFields\": additional_model_fields\n",
    "}\n",
    "\n",
    "# Ensure `bedrock_client` is properly initialized (replace with actual initialization)\n",
    "# For example: bedrock_client = SomeBedrockClientLibrary.initialize()\n",
    "\n",
    "# try:\n",
    "#     # Making the API call\n",
    "#     response = bedrock_client.converse(**converse_api_params)\n",
    "    \n",
    "#     # Print response content\n",
    "#     # Adjust the response access according to the actual response structure\n",
    "#     # Assuming the response structure: response['output']['message']['content']\n",
    "    \n",
    "#     if 'output' in response and 'message' in response['output'] and 'content' in response['output']['message']:\n",
    "#         print(response['output']['message']['content'])\n",
    "#     else:\n",
    "#         print(\"Unexpected response structure:\", response)\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(f\"An error occurred: {e}\")\n",
    "\n",
    "try:\n",
    "    # Making the API call\n",
    "    bedrock_client = SomeBedrockClientLibrary.initialize()\n",
    "    response = bedrock_client.converse(**converse_api_params)\n",
    "    \n",
    "    \n",
    "    # Print response content\n",
    "    # Adjust the response access according to the actual response structure\n",
    "    # Assuming the response structure: response['output']['message']['content']\n",
    "    \n",
    "    if 'output' in response and 'message' in response['output'] and 'content' in response['output']['message']:\n",
    "        print(response['output']['message']['content'])\n",
    "    else:\n",
    "        print(\"Unexpected response structure:\", response)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are some critical thinking questions to further explore why the sky is blue:\n",
      "\n",
      "- What is the composition of the Earth's atmosphere, and how does that affect the way light interacts with it?\n",
      "- How do different wavelengths of light behave as they pass through the atmosphere?\n",
      "- What is the process known as Rayleigh scattering, and how does it contribute to the blue appearance of the sky?\n",
      "- How do factors like the time of day, weather conditions, and location on Earth influence the appearance of the sky?\n",
      "- What other celestial bodies exhibit different colors in their skies, and what does that tell us about their atmospheric compositions?\n",
      "- How have our scientific understandings of the sky's color changed over time as our knowledge of optics and atmospheric science has advanced?\n",
      "\n",
      "Exploring these types of questions can help deepen our understanding of the physical principles behind the sky's blue appearance. Let me know if you have any other\n"
     ]
    }
   ],
   "source": [
    "# System prompt\n",
    "SYSTEM_PROMPT = \"Your answer should always be a series of critical thinking questions that further the conversation (do not provide answers to your questions). Do not actually answer the user question.\"\n",
    "\n",
    "# Prompt\n",
    "PROMPT = \"Why is the sky blue?\"\n",
    "\n",
    "# Print Claude's response\n",
    "print(get_completion(PROMPT, SYSTEM_PROMPT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
