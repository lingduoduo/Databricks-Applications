{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix 10.2.4: Tool choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Claude API supports a parameter called `tool_choice` that allows you to specify how you want Claude to call tools. In this notebook, we'll take a look at how it works and when to use it.\n",
    "\n",
    "When working with the `tool_choice` parameter, we have three possible options: \n",
    "\n",
    "* `auto` allows Claude to decide whether to call any provided tools or not.\n",
    "* `any` tells Claude that it must use one of the provided tools, but doesn't force a particular tool.\n",
    "* `tool` allows us to force Claude to always use a particular tool.\n",
    "\n",
    "\n",
    "This diagram illustrates how each option works: \n",
    "\n",
    "![tool_choice.png](./images/tool_choice.png)\n",
    "\n",
    "Let's take a look at each option in detail. We'll start by importing the Anthropic SDK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU pip\n",
    "%pip install -qUr requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "from datetime import datetime\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "session = boto3.Session()\n",
    "region = session.region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using modelId: anthropic.claude-3-sonnet-20240229-v1:0\n",
      "Using region:  {'us-west-2'}\n"
     ]
    }
   ],
   "source": [
    "modelId = 'anthropic.claude-3-sonnet-20240229-v1:0'\n",
    "#modelId = 'anthropic.claude-3-haiku-20240307-v1:0'\n",
    "\n",
    "print(f'Using modelId: {modelId}')\n",
    "print(f'Using region: ', {region})\n",
    "\n",
    "bedrock_client = boto3.client(service_name = 'bedrock-runtime', region_name = region,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto\n",
    "\n",
    "Setting `tool_choice` to `auto` allows the model to automatically decide whether to use tools or not.  This is the default behavior when working with tools if you don't use the `tool_choice` parameter at all.\n",
    "\n",
    "To demonstrate this, we're going to provide Claude with a fake web search tool. We will ask Claude questions, some of which would require calling the web search tool and others which Claude should be able to answer on its own.\n",
    "\n",
    "Let's start by defining a tool called `web_search`.  Please note, to keep this demo simple, we're not actually searching the web here.\n",
    "\n",
    "We also set `toolChoice` to `auto`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def web_search(topic):\n",
    "    print(f\"pretending to search the web for {topic}\")\n",
    "\n",
    "toolConfig = {'tools': [],\n",
    "        \"toolChoice\": {\n",
    "        \"auto\":{},\n",
    "    }\n",
    "}\n",
    "\n",
    "toolConfig['tools'].append({\n",
    "      \"toolSpec\": {\n",
    "        \"name\": \"web_search\",\n",
    "        \"description\": \"A tool to retrieve up to date information on a given topic by searching the web. Only search the web for queries that you can not confidently answer.\",\n",
    "        \"inputSchema\": {\n",
    "          \"json\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "              \"topic\": {\"type\": \"string\", \"description\": \"The topic to search the web for\"}\n",
    "            },\n",
    "            \"required\": [\"topic\"]\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we write a function that accepts a `user_query` and passes it along to Claude, along with the `web_search_tool`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the complete function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "def chat_with_web_search(user_query):\n",
    "    messages = [{\"role\": \"user\", \"content\": [{\"text\": user_query}]}]\n",
    "\n",
    "    system_prompt=f\"\"\"\n",
    "    Answer as many questions as you can using your existing knowledge.  \n",
    "    Only search the web for queries that you can not confidently answer.\n",
    "    Today's date is {date.today().strftime(\"%B %d %Y\")}\n",
    "    If you think a user's question involves something in the future that hasn't happened yet, use the search tool.\n",
    "    \"\"\"\n",
    "\n",
    "    converse_api_params = {\n",
    "        \"modelId\": modelId,\n",
    "        \"system\": [{\"text\": system_prompt}],\n",
    "        \"messages\": messages,\n",
    "        \"inferenceConfig\": {\"temperature\": 0.0, \"maxTokens\": 1000},\n",
    "        \"toolConfig\":toolConfig\n",
    "    }\n",
    "\n",
    "    response = bedrock_client.converse(**converse_api_params)\n",
    "\n",
    "    stop_reason = response['stopReason']\n",
    "\n",
    "    if stop_reason == \"end_turn\":\n",
    "        print(\"Claude did NOT call a tool\")\n",
    "        print(f\"Assistant: {stop_reason}\")\n",
    "    elif stop_reason == \"tool_use\":\n",
    "        print(\"Claude wants to use a tool\")\n",
    "        print(stop_reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a question Claude should be able to answer without using the tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude did NOT call a tool\n",
      "Assistant: end_turn\n"
     ]
    }
   ],
   "source": [
    "chat_with_web_search(\"What color is the sky?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we ask \"What color is the sky?\", Claude does not use the tool.  Let's try asking something that Claude should use the web search tool to answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude wants to use a tool\n",
      "tool_use\n"
     ]
    }
   ],
   "source": [
    "chat_with_web_search(\"Who won the 2024 Miami Grand Prix?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we ask \"Who won the 2024 Miami Grand Prix?\", Claude uses the web search tool! \n",
    "\n",
    "Let's try a few more examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude did NOT call a tool\n",
      "Assistant: end_turn\n"
     ]
    }
   ],
   "source": [
    "# Claude should NOT need to use the tool for this:\n",
    "chat_with_web_search(\"Who won the Superbowl in 2022?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude wants to use a tool\n",
      "tool_use\n"
     ]
    }
   ],
   "source": [
    "# Claude SHOULD use the tool for this:\n",
    "chat_with_web_search(\"Who won the Superbowl in 2024?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your prompt matters!\n",
    "\n",
    "When working with `toolChoice` set to `auto`, it's important that you spend time to write a detailed prompt.  Often, Claude can be over-eager to call tools.  Writing a detailed prompt helps Claude determine when to call a tool and when not to.  In the above example, we included specific instructions in the system prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt=f\"\"\"\n",
    "    Answer as many questions as you can using your existing knowledge.\n",
    "    Only search the web for queries that you can not confidently answer.\n",
    "    Today's date is {date.today().strftime(\"%B %d %Y\")}\n",
    "    If you think a user's question involves something in the future that hasn't happened yet, use the search tool.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forcing a specific tool\n",
    "\n",
    "We can force Claude to use a particular tool using `toolChoice`.  In the example below, we've defined two simple tools: \n",
    "* `print_sentiment_scores` - a tool that \"tricks\" Claude into generating well-structured JSON output containing sentiment analysis data.  For more info on this approach, see [Extracting Structured JSON using Claude and Tool Use](https://github.com/anthropics/anthropic-cookbook/blob/main/tool_use/extracting_structured_json.ipynb) in the Anthropic Cookbook.\n",
    "* `calculator` - a very simple calculator tool that takes two numbers and adds them together .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to write a function called `analyze_tweet_sentiment` that takes in a tweet and uses Claude to print a basic sentiment analysis of that tweet.  Eventually we will \"force\" Claude to use the `print_sentiment_scores` tool, but we'll start by showing what happens when we **do not** force the tool use. \n",
    "\n",
    "In this first \"bad\" version of the `analyze_tweet_sentiment` function, we provide Claude with both tools. For the sake of comparison, we'll start by setting `toolChoice` to `auto`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our toolConfig\n",
    "toolConfig = {'tools': [],\n",
    "        \"toolChoice\": {\n",
    "        \"auto\":{},\n",
    "    }\n",
    "}\n",
    "\n",
    "# append our print_sentiment_scores tool\n",
    "toolConfig['tools'].append({\n",
    "    \"toolSpec\": {\n",
    "      \"name\": \"print_sentiment_scores\",\n",
    "      \"description\": \"Prints the sentiment scores of a given tweet or piece of text.\",\n",
    "      \"inputSchema\": {\n",
    "        \"json\": {\n",
    "          \"type\": \"object\",\n",
    "          \"properties\": {\n",
    "            \"positive_score\": {\"type\": \"number\",\"description\": \"The positive sentiment score, ranging from 0.0 to 1.0.\"},\n",
    "            \"negative_score\": {\"type\": \"number\",\"description\": \"The negative sentiment score, ranging from 0.0 to 1.0.\"},\n",
    "            \"neutral_score\": {\"type\": \"number\",\"description\": \"The neutral sentiment score, ranging from 0.0 to 1.0.\"}\n",
    "          },\n",
    "          \"required\": [\"positive_score\", \"negative_score\", \"neutral_score\"]\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  })\n",
    "\n",
    "# Append our Calculator tool\n",
    "toolConfig['tools'].append({\n",
    "    \"toolSpec\": {\n",
    "      \"name\": \"calculator\",\n",
    "      \"description\": \"Adds two numbers\",\n",
    "      \"inputSchema\": {\n",
    "        \"json\": {\n",
    "          \"type\": \"object\",\n",
    "          \"properties\": {\n",
    "            \"num1\": {\"type\": \"number\", \"description\": \"first number to add\"},\n",
    "            \"num2\": {\"type\": \"number\", \"description\": \"second number to add\"}\n",
    "          },\n",
    "          \"required\": [\"num1\", \"num2\"]\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that we are deliberately not providing Claude with a well-written prompt, to make it easier to see the impact of forcing the use of a particular tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def analyze_tweet_sentiment(query):\n",
    "    messages = [{\"role\": \"user\", \"content\": [{\"text\": query}]}]\n",
    "\n",
    "    converse_api_params = {\n",
    "        \"modelId\": modelId,\n",
    "        \"system\": [{\"text\": system_prompt}],\n",
    "        \"messages\": messages,\n",
    "        \"inferenceConfig\": {\"temperature\": 0.0, \"maxTokens\": 1000},\n",
    "        \"toolConfig\":toolConfig,\n",
    "    }\n",
    "    response = bedrock_client.converse(**converse_api_params)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens when we call the function with the tweet `Holy cow, I just made the most incredible meal!`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'RequestId': 'd0de4cb3-4f11-4586-bcfc-95f2aebeb49f', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Wed, 28 Aug 2024 03:29:54 GMT', 'content-type': 'application/json', 'content-length': '494', 'connection': 'keep-alive', 'x-amzn-requestid': 'd0de4cb3-4f11-4586-bcfc-95f2aebeb49f'}, 'RetryAttempts': 0}, 'output': {'message': {'role': 'assistant', 'content': [{'text': \"That's great to hear! I'm glad you enjoyed your meal so much. Since you didn't ask a specific question, I don't have enough context to provide a more substantive response. But I'd be happy to discuss your cooking experience further if you have any other details to share about the incredible meal you prepared.\"}]}}, 'stopReason': 'end_turn', 'usage': {'inputTokens': 495, 'outputTokens': 68, 'totalTokens': 563}, 'metrics': {'latencyMs': 2057}}\n"
     ]
    }
   ],
   "source": [
    "analyze_tweet_sentiment(\"Holy cow, I just made the most incredible meal!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claude does not call our `print_sentiment_scores` tool and instead responds directly with:\n",
    "> \"That's great to hear! I don't actually have the capability to assess sentiment from text, but it sounds like you're really excited and proud of the incredible meal you made\n",
    "\n",
    "Next, let's imagine someone tweets this: `I love my cats! I had four and just adopted 2 more! Guess how many I have now?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'RequestId': '4b95ff9b-c158-4beb-b9b6-e64f1c5b8076', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Wed, 28 Aug 2024 03:29:56 GMT', 'content-type': 'application/json', 'content-length': '442', 'connection': 'keep-alive', 'x-amzn-requestid': '4b95ff9b-c158-4beb-b9b6-e64f1c5b8076'}, 'RetryAttempts': 0}, 'output': {'message': {'role': 'assistant', 'content': [{'text': 'No need to use any tools for this simple math problem. If you had 4 cats originally and adopted 2 more, then the total number of cats you have now is:'}, {'toolUse': {'toolUseId': 'tooluse_4UkesfluRqSdZqsluXghuA', 'name': 'calculator', 'input': {'num1': 4, 'num2': 2}}}]}}, 'stopReason': 'tool_use', 'usage': {'inputTokens': 508, 'outputTokens': 109, 'totalTokens': 617}, 'metrics': {'latencyMs': 2161}}\n"
     ]
    }
   ],
   "source": [
    "analyze_tweet_sentiment(\"I love my cats! I had four and just adopted 2 more! Guess how many I have now?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claude wants to call the calculator tool:\n",
    "\n",
    "> {'toolUse': {'toolUseId': 'tooluse_oyzX9vToT468sAwe_A99EA', **'name': 'calculator', 'input': {'num1': 4, 'num2': 2}**}}]}}, 'stopReason': 'tool_use'{'toolUse': {'toolUseId': 'tooluse_oyzX9vToT468sAwe_A99EA', 'name': 'calculator', 'input': {'num1': 4, 'num2': 2}}}]}}, 'stopReason': 'tool_use'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, this current implementation is not doing what we want (mostly because we set it up to fail). \n",
    "\n",
    "So let's force Claude to **always** use the `print_sentiment_scores` tool by updating `toolChoice`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolConfig['toolChoice'] = {\n",
    "    \"tool\": {\n",
    "        \"name\": \"print_sentiment_scores\"}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to setting `type` to `tool`, we must provide a particular tool name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_tweet_sentiment(query):\n",
    "    messages = [{\"role\": \"user\", \"content\": [{\"text\": query}]}]\n",
    "\n",
    "    converse_api_params = {\n",
    "        \"modelId\": modelId,\n",
    "        \"system\": [{\"text\": system_prompt}],\n",
    "        \"messages\": messages,\n",
    "        \"inferenceConfig\": {\"temperature\": 0.0, \"maxTokens\": 1000},\n",
    "        \"toolConfig\":toolConfig,\n",
    "    }\n",
    "    response = bedrock_client.converse(**converse_api_params)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we try prompting Claude with the same prompts from earlier, it's always going to call the `print_sentiment_scores` tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'RequestId': '9453906d-44ee-4923-9ea0-35d94ac91e54', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Wed, 28 Aug 2024 03:29:59 GMT', 'content-type': 'application/json', 'content-length': '335', 'connection': 'keep-alive', 'x-amzn-requestid': '9453906d-44ee-4923-9ea0-35d94ac91e54'}, 'RetryAttempts': 0}, 'output': {'message': {'role': 'assistant', 'content': [{'toolUse': {'toolUseId': 'tooluse_R3RfJCVgS3O81p2pfCJAxA', 'name': 'print_sentiment_scores', 'input': {'positive_score': 0.9, 'negative_score': 0.1, 'neutral_score': 0.0}}}]}}, 'stopReason': 'tool_use', 'usage': {'inputTokens': 593, 'outputTokens': 79, 'totalTokens': 672}, 'metrics': {'latencyMs': 3181}}\n"
     ]
    }
   ],
   "source": [
    "analyze_tweet_sentiment(\"Holy cow, I just made the most incredible meal!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claude calls our `print_sentiment_scores` tool:\n",
    "\n",
    "> [{'toolUse': {'toolUseId': 'tooluse_EZnn27PHRXWfo7JR8FWkDw', **'name': 'print_sentiment_scores',** 'input': {'positive_score': 0.9, 'negative_score': 0.1, 'neutral_score': 0.0}}}][{'toolUse': {'toolUseId': 'tooluse_EZnn27PHRXWfo7JR8FWkDw', 'name': 'print_sentiment_scores', 'input': {'positive_score': 0.9, 'negative_score': 0.1, 'neutral_score': 0.0}}}]\n",
    "\n",
    "Even if we try to trip up Claude with a \"Math-y\" tweet, it still always calls the `print_sentiment_scores` tool:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ResponseMetadata': {'RequestId': 'f1b43d8e-8908-4944-bd72-e7bc66a7108a', 'HTTPStatusCode': 200, 'HTTPHeaders': {'date': 'Wed, 28 Aug 2024 03:30:01 GMT', 'content-type': 'application/json', 'content-length': '335', 'connection': 'keep-alive', 'x-amzn-requestid': 'f1b43d8e-8908-4944-bd72-e7bc66a7108a'}, 'RetryAttempts': 0}, 'output': {'message': {'role': 'assistant', 'content': [{'toolUse': {'toolUseId': 'tooluse_zu8yfPb5QSeo0mIkqN420A', 'name': 'print_sentiment_scores', 'input': {'positive_score': 0.8, 'negative_score': 0.1, 'neutral_score': 0.1}}}]}}, 'stopReason': 'tool_use', 'usage': {'inputTokens': 606, 'outputTokens': 79, 'totalTokens': 685}, 'metrics': {'latencyMs': 1545}}\n"
     ]
    }
   ],
   "source": [
    "analyze_tweet_sentiment(\"I love my cats! I had four and just adopted 2 more! Guess how many I have now?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we're forcing Claude to call our `print_sentiment_scores` tool, we should still employ some basic prompt engineering to give Claude better task context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_tweet_sentiment(query):\n",
    "    prompt = f\"\"\"\n",
    "    Analyze the sentiment in the following tweet:\n",
    "    <tweet>{query}</tweet>\"\"\"\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": [{\"text\": prompt}]}]\n",
    "\n",
    "    converse_api_params = {\n",
    "        \"modelId\": modelId,\n",
    "        \"system\": [{\"text\": system_prompt}],\n",
    "        \"messages\": messages,\n",
    "        \"inferenceConfig\": {\"temperature\": 0.0, \"maxTokens\": 1000},\n",
    "        \"toolConfig\":toolConfig,\n",
    "    }\n",
    "    response = bedrock_client.converse(**converse_api_params)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Any\n",
    "\n",
    "The final option for `toolChoice` is `any`, which allows us to tell Claude, \"You must call a tool, but you can pick which one.\"  Imagine we want to create a SMS chatbot using Claude.  The only way for this chatbot to actually \"communicate\" with a user is via SMS text message. \n",
    "\n",
    "In the example below, we make a very simple text-messaging assistant that has access to two tools:\n",
    "* `send_text_to_user` - sends a text message to a user.\n",
    "* `get_customer_info` - looks up customer data based on a username.\n",
    "\n",
    "The idea is to create a chatbot that always calls one of these tools and never responds with a non-tool response.  In all situations, Claude should either respond back by trying to send a text message or calling `get_customer_info` to get more customer information. To ensure this, we set `toolChoice` to `any`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolConfig = {'tools': [],\n",
    "        \"toolChoice\": {\n",
    "        \"any\":{},\n",
    "    }\n",
    "}\n",
    "\n",
    "toolConfig['tools'].append({\n",
    "      \"toolSpec\": {\n",
    "        \"name\": \"send_text_to_user\",\n",
    "        \"description\": \"Sends a text message to a user\",\n",
    "        \"inputSchema\": {\n",
    "          \"json\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "              \"text\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The piece of text to be sent to the user via text message\"}\n",
    "            },\n",
    "            \"required\": [\"text\"]\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    })\n",
    "\n",
    "toolConfig['tools'].append({\n",
    "      \"toolSpec\": {\n",
    "        \"name\": \"get_customer_info\",\n",
    "        \"description\": \"gets information on a customer based on the customer's username.  Response includes email, username, and previous purchases. Only call this tool once a user has provided you with their username\",\n",
    "        \"inputSchema\": {\n",
    "          \"json\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "              \"username\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The username of the user in question. \"}\n",
    "            },\n",
    "            \"required\": [\"username\"]\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#toolConfig # Optional uncomment to see the updated toolConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_text_to_user(text):\n",
    "    # Sends a text to the user\n",
    "    # We'll just print out the text to keep things simple:\n",
    "    print(f\"TEXT MESSAGE SENT: {text}\")\n",
    "\n",
    "def get_customer_info(username):\n",
    "    return {\n",
    "        \"username\": username,\n",
    "        \"email\": f\"{username}@email.com\",\n",
    "        \"purchases\": [\n",
    "            {\"id\": 1, \"product\": \"computer mouse\"},\n",
    "            {\"id\": 2, \"product\": \"screen protector\"},\n",
    "            {\"id\": 3, \"product\": \"usb charging cable\"},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "All your communication with a user is done via text message.\n",
    "Only call tools when you have enough information to accurately call them.  \n",
    "Do not call the get_customer_info tool until a user has provided you with their username. This is important.\n",
    "If you do not know a user's username, simply ask a user for their username.\n",
    "\"\"\"\n",
    "\n",
    "def sms_chatbot(user_message):\n",
    "    messages = [{\"role\": \"user\", \"content\": [{\"text\": user_message}]}]\n",
    "\n",
    "    converse_api_params = {\n",
    "        \"modelId\": modelId,\n",
    "        \"system\": [{\"text\": system_prompt}],\n",
    "        \"messages\": messages,\n",
    "        \"inferenceConfig\": {\"temperature\": 0.0, \"maxTokens\": 1000},\n",
    "        \"toolConfig\":toolConfig,\n",
    "    }\n",
    "\n",
    "    response = bedrock_client.converse(**converse_api_params)\n",
    "\n",
    "    if(response['stopReason'] == \"tool_use\"):\n",
    "        tool_use = response['output']['message']['content'][-1]\n",
    "        tool_name = tool_use['toolUse']['name']\n",
    "        tool_inputs = tool_use['toolUse']['input']\n",
    "        print(f\"=======Claude Wants To Call The {tool_name} Tool=======\")\n",
    "        if tool_name == \"send_text_to_user\":\n",
    "            send_text_to_user(tool_inputs[\"text\"])\n",
    "        elif tool_name == \"get_customer_info\":\n",
    "            print(get_customer_info(tool_inputs[\"username\"]))\n",
    "        else:\n",
    "            print(\"Oh dear, that tool doesn't exist!\")\n",
    "            \n",
    "    else:\n",
    "        print(\"No tool was called. This shouldn't happen!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======Claude Wants To Call The send_text_to_user Tool=======\n",
      "TEXT MESSAGE SENT: Hello! I'm an AI assistant. How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "sms_chatbot(\"Hey there! How are you?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claude responds back by calling the `send_text_to_user` tool.\n",
    "\n",
    "Next, we'll ask Claude something a bit trickier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======Claude Wants To Call The send_text_to_user Tool=======\n",
      "TEXT MESSAGE SENT: Sure, I can help you look up your order information. To get started, please provide me with your username so I can retrieve your account details.\n"
     ]
    }
   ],
   "source": [
    "sms_chatbot(\"I need help looking up an order\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claude wants to send a text message, asking a user to provide their username.\n",
    "\n",
    "Now, let's see what happens when we provide Claude with our username:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======Claude Wants To Call The get_customer_info Tool=======\n",
      "{'username': 'jenny76', 'email': 'jenny76@email.com', 'purchases': [{'id': 1, 'product': 'computer mouse'}, {'id': 2, 'product': 'screen protector'}, {'id': 3, 'product': 'usb charging cable'}]}\n"
     ]
    }
   ],
   "source": [
    "sms_chatbot(\"I need help looking up an order.  My username is jenny76\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Claude calls the `get_customer_info` tool, just as we hoped! \n",
    "\n",
    "Even if we send Claude a gibberish message, it will still call one of our tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=======Claude Wants To Call The send_text_to_user Tool=======\n",
      "TEXT MESSAGE SENT: I'm afraid I don't understand your query. Could you please rephrase it or provide more context? I'll do my best to assist you once I understand what you need.\n"
     ]
    }
   ],
   "source": [
    "sms_chatbot(\"askdj aksjdh asjkdbhas kjdhas 1+1 ajsdh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
