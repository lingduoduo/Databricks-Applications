{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f4f2e31e-971e-4b92-942c-2707b24465fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3ff75cb-b0f3-4ce6-a7bc-2245170593de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 3.1 - Deploying the Databricks Assets\n",
    "\n",
    "In Databricks, you have several options for deploying your Databricks Assets such as the UI, REST APIs, Databricks CLI, Databricks SDK or Databricks Asset Bundles (DABs). Databricks recommends Databricks Asset Bundles for creating, developing, deploying, and testing jobs and other Databricks resources as source code. \n",
    "\n",
    "In this demonstration, we will deploy our project and explore the job and pipeline using the Workflows UI. Then, we will examine the Workflow JSON and YAML structures and discuss how we can use these within our CI/CD process.\n",
    "\n",
    "## Objectives\n",
    "- Deploy Databricks assets using the Databricks SDK. \n",
    "- Analyze the Workflow JSON and YAML definitions for jobs and tasks, and explore their role in enabling automated deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5c4914e-3add-415c-b5e5-daee48445068",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "1. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "  - In the drop-down, select **More**.\n",
    "\n",
    "  - In the **Attach to an existing compute resource** pop-up, select the first drop-down. You will see a unique cluster name in that drop-down. Please select that cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "1. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "1. Wait a few minutes for the cluster to start.\n",
    "\n",
    "1. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52eb3418-3c6f-4894-8d1a-91eefe3aadf8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## A. Classroom Setup\n",
    "\n",
    "Run the following cell to configure your working environment for this course. \n",
    "\n",
    "**NOTE:** The `DA` object is only used in Databricks Academy courses and is not available outside of these courses. It will dynamically reference the information needed to run the course.\n",
    "\n",
    "##### The notebook \"2.1 - Modularizing PySpark Code - Required\" sets up the catalogs for this course. If you have not run this notebook, the catalogs will not be available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e13418da-414e-42c9-ab12-8ab185a3e828",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../Includes/Classroom-Setup-3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f07e93b0-31d5-424f-bfde-33ac51baefdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## B. Create the Databricks Job with Notebooks, Python Files and DLT\n",
    "\n",
    "1. In this section, we will create the job for our project. The job will contain the following tasks:\n",
    "- Unit tests\n",
    "- DLT pipeline with the ETL pipeline and integration tests \n",
    "- Final data visualization deliverable for this project\n",
    "\n",
    "**FINAL JOB**\n",
    "\n",
    "![Final SDK Workflow](../Includes/images/05_final_sdk_workflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca7b4078-cbc3-4684-b5d3-eb3724f6279e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. During development, it's beneficial to build out your Workflow and/or DLT Pipeline using the UI. The UI provides an easy way to build your desired job, and it generates the necessary JSON or YAML files to start automating your deployment across different environments.\n",
    "\n",
    "    **NOTE:** In the cell below, we will create the job using custom functions provided by Databricks Academy, which leverages the Databricks SDK behind the scenes. **This approach saves time in class by avoiding the need to manually create the Workflow using the UI for the demonstration. Workflows are a prerequisite for this course.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4fb93a45-e29d-4bf3-be94-a66577148811",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## Confirm the pipeline from the previous demonstration exists. If not, create the DLT pipeline and store the ID\n",
    "my_pipeline_id = obtain_pipeline_id_or_create_if_not_exists()\n",
    "print(my_pipeline_id)\n",
    "\n",
    "# ## Create the job\n",
    "create_demo_5_job(my_pipeline_id = my_pipeline_id, job_name = f'Dev Workflow Using the SDK_{DA.catalog_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55285ab4-4d89-4be7-92c9-4478dea9cc83",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Complete the following steps to view and run the new job:\n",
    "\n",
    "    a. In the far-left navigation bar, right-click on **Workflows** and select *Open Link in New Tab*.\n",
    "\n",
    "    b. In the **Jobs** tab, you should see a job named **Dev Workflow Using the SDK_user_name**.\n",
    "\n",
    "    c. Select the job **Dev Workflow Using the SDK_user_name**.\n",
    "\n",
    "    d. Select on **Run now** to run the job.\n",
    "\n",
    "    e. Leave the job open."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "521654e3-4c11-4bc3-b6ad-437386a3e964",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "4. While the job is running, explore the job tasks. The job will take between 5-7 minutes to complete. Navigate to the job **Runs** tab. Here you should see the job executing.\n",
    "\n",
    "<br></br>\n",
    "\n",
    "**Job Task Descriptions**\n",
    "![Final SDK Workflow Desc](../Includes/images/05_Final_Workflow_Desc.png)\n",
    "\n",
    "#####4a. Task 1: Unit_Tests\n",
    "\n",
    "   a. On the job **Runs** tab, right click on the square for **Unit_Tests** and select *Open Link in New Tab*.\n",
    "\n",
    "   b. Notice the **Run Unit Tasks** notebook executes the unit tests we created earlier.\n",
    "\n",
    "   c. Close the tab.\n",
    "\n",
    "<br></br>\n",
    "\n",
    "#####4b. Task 2: Health_ETL\n",
    "\n",
    "   a. Select the **Tasks** tab and select the **Health_ETL** task*.\n",
    "\n",
    "   b. In the **Task** section find the **Pipeline** value and select the icon to the right of the pipeline name to open the pipeline.\n",
    "\n",
    "  **NOTE:** If the DLT pipeline has already completed, simply select the pipeline link.\n",
    "\n",
    "   c. Notice the pipeline executes (will execute) the ETL pipeline we created earlier.\n",
    "\n",
    "   d. Close the DLT pipeline.\n",
    "\n",
    "<br></br>\n",
    "\n",
    "#####4c. Task 3: Visualization\n",
    "   a. Select the job **Runs** tab, right click on the square for **Visualization** and select *Open Link in New Tab*.\n",
    "\n",
    "   b. Notice the **Final Visualization** notebook creates the final visualization for the project.\n",
    "\n",
    "   c. Close the tab.\n",
    "\n",
    "\n",
    "Leave the job open while it continues to run and continue the next steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66f97e10-ccfa-4f7a-bf51-6824e41e61d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "5. Complete the following to view the JSON file to deploy this job:\n",
    "\n",
    "   a. Navigate back to the main Workflow job by selecting the **Tasks** tab.\n",
    "\n",
    "   b. At the top right of the job, select the kebab menu (three ellipsis icon near the **Run now** button).\n",
    "\n",
    "   c. Select **View JSON**.\n",
    "\n",
    "   d. Notice that you can view the JSON definition for the job for use with the REST API. This is a great way to easily obtain the necessary values to begin automating your deployment for the REST API (the SDK values are similar).\n",
    "\n",
    "   e. Close the **Job JSON** popup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8715c2b-d784-4778-ae1a-4abe06f69571",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "6. Complete the following to view the YAML file to deploy this job:\n",
    "\n",
    "   a. Confirm you are on the **Tasks** tab.\n",
    "\n",
    "   b. At the top right of the job, select the kebab menu (three ellipsis icon near the **Run now** button).\n",
    "\n",
    "   c. Select **Edit as YAML**.\n",
    "\n",
    "   d. Notice that you can view job as the YAML for the job for. This is a great way to easily obtain the necessary values for the YAML deployment (this YAML file is extremely help when deploying using **Databricks Asset Bundles**).\n",
    "\n",
    "   e. In the top right select **Close editor**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f407a0b9-e327-49c8-b500-fde3ef032d67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "7. The job should be completed by now. View the completed job and confirm the three tasks completed successfully. Feel free to view the completed tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "845189f6-fe9b-4ae4-a024-f1aa4510828a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "8. Complete the following steps to view the Databricks SDK code to create the Workflow.\n",
    "\n",
    "    a. SDK Code: **[../Includes/Classroom-Setup-3.1]($../Includes/Classroom-Setup-3.1)**\n",
    "\n",
    "    b. Scroll down to cell 4: `def create_demo_5_job(my_pipeline_id, job_name)` \n",
    "    \n",
    "    c. Notice that amount of Python code used to create the Job to deploy our development code. \n",
    "    \n",
    "    **NOTE:** Details of the Databricks SDK code is beyond the scope of this course.\n",
    "\n",
    "While the SDK provides low-level control over your deployment, it also requires significant time and effort to write all the necessary code.  \n",
    "\n",
    "In this example, we are only deploying the development job. Additional modifications will be needed to deploy both the staging and production jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e5173dc-e43f-47b6-8ba5-3972e9be7f94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Next Steps for CI/CD\n",
    "\n",
    "Think about the following for the deploying this project utilizing the entire CI/CD process:\n",
    "- How will I automatically deploy the databricks assets to run for **dev**, **stage**, and **prod** environments?\n",
    "- How do I parameterize all the values I need based on the target environment?\n",
    "- How do I configure the necessary variables for the DLT pipeline during each deployment?\n",
    "- How do I maintain all of the code?\n",
    "- How do I automate this entire process?\n",
    "- How can I setup continuous integration and continuous delivery or deployment (CI/CD) system, such as GitHub Actions, to automatically run your unit tests whenever your code changes? For an example, see the coverage of GitHub Actions in [Software engineering best practices for notebooks](https://docs.databricks.com/en/notebooks/best-practices.html). \n",
    "\n",
    "Next Steps: Learn more about [**Databricks Asset Bundles (DABs)**](https://docs.databricks.com/en/dev-tools/bundles/index.html) for deploying your Databricks Assets!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9320cf3a-92b7-48c6-af0a-62962cb888dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "&copy; 2025 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the \n",
    "<a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/><a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | \n",
    "<a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | \n",
    "<a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "3.1 - Deploying the Databricks Assets",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}