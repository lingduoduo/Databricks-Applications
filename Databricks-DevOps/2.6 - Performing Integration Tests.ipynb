{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dea244f6-57cb-45c6-8bdd-8b61bfdfa00f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4586a97f-71e1-4f41-abc7-23274584cd04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "https://docs.databricks.com/aws/en/dlt/expectations#manage-data-quality-with-pipeline-expectations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8391b629-d900-468b-ae71-d670edf6a49f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# 2.6 - Performing Integration Tests\n",
    "\n",
    "Integration tests for data engineering ensures that different components of the data pipeline, such as data ingestion, transformation, storage, and retrieval, work together seamlessly in a real-world environment. These tests validate the flow of data across systems, checking for issues like data consistency, format mismatches, and processing errors when components interact as expected.\n",
    "\n",
    "There are multiple ways to implement integration tests within Databricks:\n",
    "\n",
    "1. **Delta Live Tables (DLT)**: With DLT, you can use expectations to check pipeline’s results.\n",
    "    - [Manage data quality with pipeline expectations](https://docs.databricks.com/en/delta-live-tables/expectations.html#manage-data-quality-with-pipeline-expectations)\n",
    "\n",
    "2. **Workflow Tasks**: You can also perform integration tests as a Databricks Workflow with tasks - similarly what is typically done for non-DLT code.\n",
    "\n",
    "In this demonstration, we will quickly introduce to you how to perform simple integration tests with Delta Live Tables and discuss how to implement them with Workflows. Prior knowledge of DLT and Workflows is assumed.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "- Learn how to perform integration testing in DLT pipelines using expectations.\n",
    "- Understand how to perform integration tests on data from DLT pipelines using Workflow tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e50d7f6b-b477-4339-932f-cc866d14a525",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default.\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "1. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "  - In the drop-down, select **More**.\n",
    "\n",
    "  - In the **Attach to an existing compute resource** pop-up, select the first drop-down. You will see a unique cluster name in that drop-down. Please select that cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "1. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "1. Wait a few minutes for the cluster to start.\n",
    "\n",
    "1. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b20fd8c4-1884-4b11-aae8-46d6b0918d0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## A. Classroom Setup\n",
    "\n",
    "Run the following cell to configure your working environment for this course. \n",
    "\n",
    "**NOTE:** The `DA` object is only used in Databricks Academy courses and is not available outside of these courses. It will dynamically reference the information needed to run the course.\n",
    "\n",
    "##### The notebook \"2.1 - Modularizing PySpark Code - Required\" sets up the catalogs for this course. If you have not run this notebook, the catalogs will not be available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d409c152-02b5-4f0e-aa73-ead390f48a71",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Setup"
    }
   },
   "outputs": [],
   "source": [
    "%run ../Includes/Classroom-Setup-2.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c9ed857e-7ebb-4d09-be15-f496b0d8b729",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## B. Option 1 - Delta Live Tables (DLT) Pipeline with Integration Tests\n",
    "\n",
    "In this section, we will create a DLT pipeline using the modularized functions from the `src.helpers` file, which we unit tested in the previous notebook. In the DLT pipeline, we will use these functions to create tables and then implement some simple integration tests for the output tables in our ETL pipeline for this project.\n",
    "\n",
    "- With DLT, you can use expectations to check pipeline’s results.\n",
    "  - [Manage data quality with pipeline expectations](https://docs.databricks.com/en/delta-live-tables/expectations.html#manage-data-quality-with-pipeline-expectations)\n",
    "\n",
    "  - [Expectation recommendations and advanced patterns](https://docs.databricks.com/en/delta-live-tables/expectation-patterns.html#expectation-recommendations-and-advanced-patterns)\n",
    "\n",
    "  - [Applying software development & DevOps best practices to Delta Live Table pipelines](https://www.databricks.com/blog/applying-software-development-devops-best-practices-delta-live-table-pipelines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e05f7f89-b1bd-4b0d-b226-0fc147d593af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. We will create the DLT pipeline for this project using the Databricks Academy **`DAPipelineConfig`** class, which was specifically designed for this course with the Databricks SDK. This avoids manually creating the DLT pipeline for this demo. Typically during development you would manually build the DLT pipeline with the UI during development.\n",
    "\n",
    "    **NOTE:** The Databricks SDK is outside the scope of this course. However, if you're interested in seeing the code that uses the SDK to automate building DLT pipelines in Databricks Academy, check out the **[../Includes/Classroom-Setup-Common]($../Includes/Classroom-Setup-Common)** notebook in **Cell 6**.\n",
    "\n",
    "    [Databricks SDK for Python](https://docs.databricks.com/en/dev-tools/sdk-python.html)\n",
    "\n",
    "    [Databricks SDK Documentation](https://databricks-sdk-py.readthedocs.io/en/latest/)\n",
    "\n",
    "\n",
    "![Full DLT Pipeline](../Includes/images/04_dlt_pipeline.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1209933b-5405-4f49-8c84-68c08154eb56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create the DLT pipeline for this project using the custom Databricks Academy class DAPipelineConfig that was created using the Databricks SDK. \n",
    "\n",
    "pipeline = DAPipelineConfig(pipeline_name=f\"sdk_health_etl_{DA.catalog_dev}\", \n",
    "                            catalog=f\"{DA.catalog_dev}\",\n",
    "                            schema=\"default\", \n",
    "                            pipeline_notebooks=[\n",
    "                                \"/src/dlt_pipelines/ingest-bronze-silver_dlt\", \n",
    "                                \"/src/dlt_pipelines/gold_tables_dlt\",\n",
    "                                \"/tests/integration_test/integration_tests_dlt\"\n",
    "                              ],\n",
    "                            config_variables={\n",
    "                                'target':'development', \n",
    "                                'raw_data_path': f'/Volumes/{DA.catalog_dev}/default/health'\n",
    "                              }\n",
    "                          )\n",
    "\n",
    "pipeline.create_dlt_pipeline()\n",
    "\n",
    "pipeline.start_dlt_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d54a8fbf-a3ee-4486-be26-5dbf337e4336",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "2. While the DLT pipeline is running, examine it through the UI by completing the following steps:\n",
    "\n",
    "   a. In the far left navigation pane, right-click on **Pipelines** and select *Open in a New Tab*.\n",
    "\n",
    "   b. Find your DLT pipeline named **sdk_health_etl_your_catalog_1_dev** and select it.\n",
    "\n",
    "   c. Click **Settings** at the top right.\n",
    "\n",
    "    - c1. In the **General** section notice that this DLT pipeline is using **Serverless** compute.\n",
    "\n",
    "    - c2. Scroll down to the **Advanced** section. You'll notice that the pipeline contains two **Configuration** variables:\n",
    "\n",
    "      - **target** = *'development'*\n",
    "        - This `target` variable will be modified dynamically for each deployment to **development**, **stage**, and **production**.\n",
    "\n",
    "      - **raw_data_path** = *'/Volumes/your_catalog_1_dev/default/health'*\n",
    "        - This `raw_data_path` variable will be modified dynamically for each deployment to **development data**, **stage data**, and **production data**.\n",
    "\n",
    "    - c3. Click **Cancel** at the bottom right.\n",
    "\n",
    "   d. At the top of the Pipelines select the kebab menu (three ellipses) and select **View settings YAML**. Notice that the UI provides the necessary YAML files for future deployment. We will talk more about this later. \n",
    "\n",
    "   e. In the **Pipeline details** section on the far right, you should see three notebooks being used for the **Source code**. Right-click each notebook and select *Open Link in New Tab* to examine them:\n",
    "\n",
    "    - **Notebook 1: [..../src/dlt_pipelines/ingest-bronze-silver_dlt]($../../src/dlt_pipelines/ingest-bronze-silver_dlt)** - Obtains the DLT configuration variables that setup the target and raw data, and creates the bronze and silver tables based on those variable values.\n",
    "  \n",
    "    - **Notebook 2: [..../src/dlt_pipelines/gold_tables_dlt]($../../src/dlt_pipelines/gold_tables_dlt)** - Creates the gold table.\n",
    "  \n",
    "    - **Notebook 3: [..../tests/integration_test/integration_tests_dlt]($../../tests/integration_test/integration_tests_dlt)** - Performs simple integration tests on the bronze, silver and gold tables based on the target environment.\n",
    "\n",
    "   h. Here is a diagram of the entire DLT pipeline for **development, stage and production**. Depending on the values of the **target** and **raw_data_path** configuration variables that are set, the ingest data source and integration tests will vary (dev catalog, stage catalog, prod catalog), but the ETL pipeline will remain the same.\n",
    "\n",
    "  ![Explain DLT Pipeline]( ../Includes/images/04_dlt_explain_integrations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6dfae2df-2fb0-4900-8b96-c6180668f00d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## C. Option 2 - Integration Testing with Notebooks and Databricks Workflows\n",
    "You can also perform integration testing using notebooks and add them as tasks in jobs for your pipeline. \n",
    "\n",
    "**NOTE:** We will simply review how to implement integration tests with Workflows if that is the method you prefer. The final deployment for this course uses the DLT integration tests with expectations.\n",
    "\n",
    "#### Steps to take:\n",
    "1. Create a setup notebook to handle any dynamic setup required using job parameters for your target environment and data locations.\n",
    "\n",
    "2. Create additional notebooks or files to store the integration tests you want to run as tasks.\n",
    "\n",
    "3. Organize the new notebooks or files within your **tests** folder.\n",
    "\n",
    "4. Create a Workflow. Within the Workflow:\n",
    "\n",
    "   - a. Create the necessary tables or views using DLT or code.\n",
    "\n",
    "   - b. Add tasks to set up your integration tests (e.g., setting up any dynamic job parameters that need to be set).\n",
    "\n",
    "   - c. Perform validation by using your notebooks as tasks and set the tasks to all should succeed.\n",
    "\n",
    "**NOTES:** One major drawback of this approach is that you will need to write more code for setup and validation tasks, as well as manage the job parameters to dynamically modify the code based on the target environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31984583-b381-4e6f-ad3c-949b6d8735bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Summary\n",
    "Integration testing can be performed in a variety of ways within Databricks. In this demonstration, we focused on how to perform simple integration tests using DLT expectations. We also discussed how to implement them with Workflow tasks.\n",
    "\n",
    "Depending on your specific situation, you can choose the approach that best fits your needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af5db39f-d7ac-4631-a324-10ba3370b29f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "&copy; 2025 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the \n",
    "<a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/><a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | \n",
    "<a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | \n",
    "<a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "2.6 - Performing Integration Tests",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}