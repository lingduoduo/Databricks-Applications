{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7665416-ec03-488b-b357-29717af51abd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee5678d7-6e66-45b5-a15a-5365e6877f79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# File Explosion\n",
    "We see many data engineers partitioning their tables in ways that can cause major performance issues, without improving future query performance. This is called \"over partitioning\". We'll see what that looks like in practice in this demo.\n",
    "\n",
    "##### Useful References\n",
    "- [Partitioning Recommendations](https://docs.databricks.com/en/tables/partitions.html)\n",
    "- [CREATE TABLE Syntax](https://docs.databricks.com/en/sql/language-manual/sql-ref-syntax-ddl-create-table-using.html)\n",
    "- [About ZORDER](https://docs.databricks.com/en/delta/data-skipping.html)\n",
    "- [About Liquid Clustering](https://docs.databricks.com/en/delta/clustering.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6dfb764-a12e-4dad-9a31-71582027dbcd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## REQUIRED - SELECT CLASSIC COMPUTE\n",
    "\n",
    "Before executing cells in this notebook, please select your classic compute cluster in the lab. Be aware that **Serverless** is enabled by default. If you use Serverless, errors will be returned when setting compute runtime properties.\n",
    "\n",
    "Follow these steps to select the classic compute cluster:\n",
    "\n",
    "1. Navigate to the top-right of this notebook and click the drop-down menu to select your cluster. By default, the notebook will use **Serverless**.\n",
    "\n",
    "1. If your cluster is available, select it and continue to the next cell. If the cluster is not shown:\n",
    "\n",
    "  - In the drop-down, select **More**.\n",
    "\n",
    "  - In the **Attach to an existing compute resource** pop-up, select the first drop-down. You will see a unique cluster name in that drop-down. Please select that cluster.\n",
    "\n",
    "**NOTE:** If your cluster has terminated, you might need to restart it in order to select it. To do this:\n",
    "\n",
    "1. Right-click on **Compute** in the left navigation pane and select *Open in new tab*.\n",
    "\n",
    "1. Find the triangle icon to the right of your compute cluster name and click it.\n",
    "\n",
    "1. Wait a few minutes for the cluster to start.\n",
    "\n",
    "1. Once the cluster is running, complete the steps above to select your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "162d8366-9070-4e8d-94f4-36e969aa4093",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## A. Classroom Setup\n",
    "\n",
    "Run the following cell to configure your working environment for this course. It will also set your default catalog to **dbacademy** and the default schema to your specific schema name shown below using the `USE` statements.\n",
    "<br></br>\n",
    "\n",
    "\n",
    "```\n",
    "USE CATALOG dbacademy;\n",
    "USE SCHEMA dbacademy.<your unique schema name>;\n",
    "```\n",
    "\n",
    "**NOTE:** The `DA` object is only used in Databricks Academy courses and is not available outside of these courses. It will dynamically reference the information needed to run the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "771cf997-870a-480d-92d9-4bb81ef91498",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./Includes/Classroom-Setup-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a6a5db9-a7ad-42e8-9192-1e59cb55db81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT current_catalog(), current_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c566e831-c120-4aee-84fb-a7780350d181",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## B. Disable Caching\n",
    "\n",
    "Run the following cell to set a Spark configuration variable that disables disk caching.\n",
    "\n",
    "Turning disk caching off prevents Databricks from storing cloud storage files after the first query. This makes the effect of the optimizations more apparent by ensuring that files are always pulled from cloud storage for each query.\n",
    "\n",
    "For more information, see [Optimize performance with caching on Databricks](https://docs.databricks.com/en/optimizations/disk-cache.html#optimize-performance-with-caching-on-databricks).\n",
    "\n",
    "**NOTE:** This will not work in Serverless. Please use classic compute to turn off caching. If you're using Serverless, an error will be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f73f9b8-3a99-41cc-8caa-62a9ffe92ef3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set('spark.databricks.io.cache.enabled', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef1c9aae-ba06-48c1-869f-ec3cc82db704",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## C. Process & Write IoT data\n",
    "Let's generate some fake IoT data. This first time around, we are only going to generate 2,500 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6802ca47-a52a-43fb-a1d2-d7bd223129eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df = (spark\n",
    "      .range(0, 2500)\n",
    "      .select(\n",
    "          hash('id').alias('id'), # randomize our ids a bit\n",
    "          rand().alias('value'),\n",
    "          from_unixtime(lit(1701692381 + col('id'))).alias('time') \n",
    "      ))\n",
    "\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74e8d102-fafc-442c-89fe-be527e0513da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now we'll write the data to a table partitioned by **id** (2,500 distinct values), which will result in every row being written to a separate folder for each partition. Writing 2,500 rows in this fashion will take a long time, as we are creating 2,500 partitions. Each partition will contain a folder with one file, and each file will store one row of data for each **id**, leading to the 'small file' problem.\n",
    "\n",
    "Note how long it takes to generate the table.\n",
    "\n",
    "**NOTE:** This will take about 1-2 minutes to create the table with 2,500 partitions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6443871-7893-4b14-83d6-e22ddb6601a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql('DROP TABLE IF EXISTS iot_data_partitioned')\n",
    "\n",
    "(df\n",
    " .write\n",
    " .mode('overwrite')\n",
    " .option(\"overwriteSchema\", \"true\")\n",
    " .partitionBy('id')\n",
    " .saveAsTable(\"iot_data_partitioned\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24df66bc-3143-44ec-905c-fbeeb9daf7c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Describe the history of the **iot_data_partitioned** table. Confirm the following:\n",
    "- In the **operationParameters** column, the table is partitioned by **id**.\n",
    "- In the **operationMetrics** column, the table contains 2,500 files, one parquet file for each unique partitioned **id**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "01fc777f-fd51-4799-b49c-3711a5a6707a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE HISTORY iot_data_partitioned;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d22f6f4a-0137-4d9a-93c4-2f8ccb39f446",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "You can use the `SHOW PARTITIONS` statement to list all partitions of a table. Run the code and view the results. Notice that the table is partitioned by **id** and contains 2,500 rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb3d09c8-d4ce-4bc4-8961-d2cf9593207a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SHOW PARTITIONS iot_data_partitioned;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "398e476e-d7cd-489f-ac29-f475cbc1ec8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### C1. Query the Table\n",
    "Run the two queries against the partitioned table we just created. Note the time taken to execute each query.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b500e8a-6ada-418c-b819-7546ba4cd2ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Query 1 - Filter by the partitioned id column\n",
    "**NOTE:** (1-2 second execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b4ef100-427b-47d1-92b3-1abdb62db38d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Query 1: Filtering by the partitioned column. \n",
    "SELECT * \n",
    "FROM iot_data_partitioned \n",
    "WHERE id = 519220707;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0644aede-0e08-4de0-8072-1cc2f5f684f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's see how this query performed using the Spark UI. Note in particular the amount of cloud storage requests and their associated time. To view how the query performed complete the following:\n",
    "\n",
    "1. In the cell above, expand **Spark Jobs**.\n",
    "\n",
    "2. Right click on **View** and select *Open in a New Tab*. \n",
    "\n",
    "    **NOTE:** In the Vocareum lab environment if you click **View** without opening it in a new tab the pop up window will display an error.\n",
    "\n",
    "3. In the new window, find the **Associated SQL Query** header at the top and select the number.\n",
    "\n",
    "4. Here you should see the entire query plan.\n",
    "\n",
    "5. In the query plan, scroll down to the bottom and find **PhotoScan parquet dbacademy.*your schema*.iot_data_partitioned (1)** and select the plus icon.\n",
    "\n",
    "\n",
    "![1.1-query-1-iot_partitioned.png](./Includes/images/1.1-query-1-iot_partitioned.png)\n",
    "\n",
    "#### Look at the following metrics in the Spark UI:\n",
    "\n",
    "\n",
    "| Metric    | Value    | Note    |\n",
    "|-------------|-------------|-------------|\n",
    "| cloud storage request count| 1| Refers to the number of requests made to the cloud storage systems like S3, Azure Blob, or Google Cloud Storage during job execution. This could involve multiple operations like reading metadata, accessing directories, or fetching the actual data. <br></br>Monitoring this metric helps optimize performance, reduce costs, and identify potential inefficiencies in data access patterns. |\n",
    "| cloud storage response size| 880.0B|  Indicates the total amount of data transferred from cloud storage to Spark during the execution of a job. It helps track the volume of data read or written to cloud storage, providing insights into I/O performance and potential bottlenecks related to data transfer.|\n",
    "| **files pruned** | **2,499** |Indicates the number of files that Spark skipped or ignored during a job execution. A total of 2,499 files were skipped by Spark due to pruning based on the query filtering by **id**. This is due to the table being partitioned by **id**, the queried column. Spark reads only the necessary partitions for processing and skips the other partitions.|\n",
    "| **files read** | **1**|  Indicates the number of files that Spark has actually read during job execution. Here, 1 file was read during the execution of the Spark job. Only 1 file was read because the query was executed on the partitioned **id** column. Spark only needs to read the necessary partitions(s) based on the query.\n",
    "\n",
    "#### Summary\n",
    "Because the data was partitioned by **id** and queried by the partitioned column, Spark will only read the necessary partition(s) (one partition in this example) and will skip the other partitioned files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ea01ce5-9e51-42cc-a35d-a2bcc5016497",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Query 2 - Filter by a column that is not partitioned \n",
    "**NOTE:** (5-10 second execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21a81fc4-c409-4a6b-8397-c0c4e4ae2440",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT avg(value) \n",
    "FROM iot_data_partitioned \n",
    "WHERE time >= \"2023-12-04 12:19:00\" AND\n",
    "      time <= \"2023-12-04 13:01:20\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ea3d5938-da74-466a-b28f-e1ddd81f4b21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's see how this query performed using the Spark UI. Note in particular the amount of cloud storage requests and their associated time. To view how the query performed complete the following:\n",
    "\n",
    "1. In the cell above, expand **Spark Jobs**.\n",
    "\n",
    "2. Right click on **View** and select *Open in a New Tab*. \n",
    "\n",
    "    **NOTE:** In the Vocareum lab environment if you click **View** without opening it in a new tab the pop up window will display an error.\n",
    "\n",
    "3. In the new window, find the **Associated SQL Query** header at the top and select the number.\n",
    "\n",
    "4. Here you should see the entire query plan.\n",
    "\n",
    "5. In the query plan, scroll down to the bottom and find **PhotoScan parquet dbacademy.*your schema*.iot_data_partitioned (1)** and select the plus icon.\n",
    "\n",
    "#### Look at the following metrics in the Spark UI (results may vary):\n",
    "| Metric    | Value    | Note    |\n",
    "|-------------|-------------|-------------|\n",
    "| cloud storage request count total (min, med, max)| 2500 (21, 37, 37)| Refers to the number of requests made to the cloud storage systems like S3, Azure Blob, or Google Cloud Storage during job execution. This could involve multiple operations like reading metadata, accessing directories, or fetching the actual data. <br></br>Monitoring this metric helps optimize performance, reduce costs, and identify potential inefficiencies in data access patterns. <br></br>The min, med and max represent the summary of requests made by tasks or executors. The distribution is fairly uniform across tasks or executors and there is not a large variance in the number of cloud storage requests made by each task.|\n",
    "| cloud storage response size total (min, med, max)| 2.1 MiB (18.0 KiB, 31.8 KiB, 31.8 KiB)| Indicates the total amount of data transferred from cloud storage to Spark during the execution of a job. It helps track the volume of data read or written to cloud storage, providing insights into I/O performance and potential bottlenecks related to data transfer.<br></br> The min,med and max indicate most tasks transferring between 18.0 KiB and 31.8 KiB of data, showing a relatively consistent and uniform data transfer pattern across tasks.|\n",
    "| **files pruned** | **0** |A total of 0 files were skipped by Spark due to pruning based on the query's filters. This is due to the table being partitioned by **id** but queried by the **time** column. No files were pruned in this query.|\n",
    "| **files read** | **2,500**| 2,500 files were read during the execution of the Spark job. This is because of the query was executed on the **time** column and the table is partitioned **id** column. In this query, all files were read into Spark then filtered for the necessary rows.|\n",
    "\n",
    "#### Summary\n",
    "Because the data was partitioned by **id** but queried by the **time** column, Spark read all of the files to perform the required query and filter the data to return a single row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef40c096-fa69-4c5a-b724-512643d5a08d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## D. Fixing the Problem\n",
    "\n",
    "Up to this point, we have been working with 2,500 rows of data that were partitioned in the table.\n",
    "\n",
    "We are now going to increase the volume dramatically by using 50,000,000 rows. If we had tried the code above with a dataset this large, it would have taken considerably longer to create all of the partitions (directories for each partition).\n",
    "\n",
    "As before, the following cell generates the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7ba0e5e-c769-430c-b0a9-4fa925831e05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "df = (spark\n",
    "      .range(0,50000000, 1, 32)\n",
    "      .select(\n",
    "          hash('id').alias('id'), # randomize our ids a bit\n",
    "          rand().alias('value'),\n",
    "          from_unixtime(lit(1701692381 + col('id'))).alias('time') \n",
    "      )\n",
    "    )\n",
    "\n",
    "df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "431830c2-9c86-499a-b3b4-dd69e9362682",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Now we'll create a table named **iot_data** to capture the data, **this time without partitioning**. Doing it this way accomplishes the following:\n",
    "- Takes less time to run, even on larger datasets, because we are not creating a high number of table partitions.\n",
    "- Writes fewer files (32 files for 50,000,000 rows vs. 2,500 files for 2,500 rows in the partitioned table).\n",
    "- Writes faster compared to disk partitioning because all files are in one directory instead of creating 2,500 directories.\n",
    "- Queries for one **id** in about the same time as before.\n",
    "- Filters by the **time** column much faster since it only has to query one directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e74ad84a-10d6-4de3-960e-90d5a21ab44c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql('DROP TABLE IF EXISTS iot_data')\n",
    "\n",
    "(df\n",
    " .write\n",
    " .option(\"overwriteSchema\", \"true\")\n",
    " .mode('overwrite')\n",
    " .saveAsTable(\"iot_data\")\n",
    ")\n",
    "\n",
    "display(spark.sql('SELECT count(*) FROM iot_data'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c7ae602-476b-45a0-9821-cb1d6a2b1657",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Describe the history of the **iot_data** table. Confirm the following:\n",
    "- In the **operationParameters** column, confirm the table is not partitioned.\n",
    "- In the **operationMetrics** column, confirm the table contains 32 total files in the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0ff06ee-6d26-43a0-b883-b91baec76a39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE HISTORY iot_data;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d18008d9-1a9f-4c69-8d58-348bce316dad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## E. Validate Optimization\n",
    "The next two cells repeat the queries from earlier and will put this change to the test. The first cell should run almost as fast as before, and the second cell should run much faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba6965c2-c703-4fe2-9f79-748d7523f2e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### E1. Query 1 - Filter by the id column (non partitioned table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3abfb599-e906-46cd-bd50-ce2dd39dbad4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * \n",
    "FROM iot_data \n",
    "WHERE id = 519220707"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c00d838-dedd-4d96-80a7-1b212602f041",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's see how this query performed using the Spark UI. Compare the results against the same query we performed earlier against an over-partitioned table.\n",
    "\n",
    "1. In the cell above, expand **Spark Jobs**.\n",
    "\n",
    "2. Right click on **View** and select *Open in a New Tab*. \n",
    "\n",
    "    **NOTE:** In the Vocareum lab environment if you click **View** without opening it in a new tab the pop up window will display an error.\n",
    "\n",
    "3. In the new window, find the **Associated SQL Query** header at the top and select the number.\n",
    "\n",
    "4. Here you should see the entire query plan.\n",
    "\n",
    "5. In the query plan, scroll down to the bottom and find **PhotoScan parquet dbacademy.*your schema*.iot_data (1)** and select the plus icon.\n",
    "\n",
    "#### Look at the following metrics in the Spark UI (results may vary):\n",
    "| Metric    | Value    | Note    |\n",
    "|-------------|-------------|-------------|\n",
    "| cloud storage request count total (min, med, max)| 65 (8, 8, 9)|  Refers to the number of requests made to the cloud storage systems like S3, Azure Blob, or Google Cloud Storage during job execution. This could involve multiple operations like reading metadata, accessing directories, or fetching the actual data. <br></br>Monitoring this metric helps optimize performance, reduce costs, and identify potential inefficiencies in data access patterns.<br></br>The request count distribution is quite uniform across tasks/executors, as the min, med, and max values are very close to each other (8 and 9) indicating cloud storage access was consistent during execution.|\n",
    "| cloud storage response size total (min, med, max)| 216.9 MiB (24.8 MiB, 24.8 MiB, 43.0 MiB)| Indicates the total amount of data transferred from cloud storage to Spark during the execution of a job. It helps track the volume of data read or written to cloud storage, providing insights into I/O performance and potential bottlenecks related to data transfer.|\n",
    "| files pruned | 0 |A total of 0 files were skipped by Spark due to pruning based on the query's filters. This is because no optimized saving table techniques were used for the table.|\n",
    "| files read | 32| 32 files were read during the execution of the Spark job. |\n",
    "\n",
    "#### Summary\n",
    "In this example, we had 50,000,000 rows (more than the original 2,500 rows) but only 32 files and no partition in the table. While this table had many more rows, Spark only had 32 files and no partitions to query, avoiding the small file problem we encountered in the partitioned table, enabling the query to run fast."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1d5f5055-be1f-4668-9fe1-5932efffe9c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### E2. Query 2 - Filter by the time column (non partitioned table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0c22e22-6f1d-461b-b4ca-0f24cf742398",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT avg(value) \n",
    "FROM iot_data \n",
    "WHERE time >= \"2023-12-04 12:19:00\" AND \n",
    "      time <= \"2023-12-04 13:01:20\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4d16ec0-1a0b-4f3f-b050-ec2ca0667b7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's see how this query performed using the Spark UI. Compare the results against the same query we performed earlier against an over-partitioned table.\n",
    "\n",
    "1. In the cell above, expand **Spark Jobs**.\n",
    "\n",
    "2. Right click on **View** and select *Open in a New Tab*. \n",
    "\n",
    "    **NOTE:** In the Vocareum lab environment if you click **View** without opening it in a new tab the pop up window will display an error.\n",
    "\n",
    "3. In the new window, find the **Associated SQL Query** header at the top and select the number.\n",
    "\n",
    "4. Here you should see the entire query plan.\n",
    "\n",
    "5. In the query plan, scroll down to the bottom and find **PhotoScan parquet dbacademy.*your schema*.iot_data (1)** and select the plus icon.\n",
    "\n",
    "#### Look at the following metrics in the Spark UI (results may vary):\n",
    "\n",
    "| Metric    | Value    | Note    |\n",
    "|-------------|-------------|-------------|\n",
    "| cloud storage request count| 3| Refers to the number of requests made to the cloud storage systems like S3, Azure Blob, or Google Cloud Storage during job execution. This could involve multiple operations like reading metadata, accessing directories, or fetching the actual data. |\n",
    "| cloud storage response size| \t18.4 MiB| Indicates the total amount of data transferred from cloud storage to Spark during the execution of a job. It helps track the volume of data read or written to cloud storage, providing insights into I/O performance and potential bottlenecks related to data transfer.|\n",
    "| files pruned | 31 | Spark determined  that 31 files did not contain any relevant data based on the WHERE condition filter for the **time** column. |\n",
    "| files read | 1 | Spark only read 1 of the files from cloud storage. |\n",
    "\n",
    "#### Summary\n",
    "In this example, we had 50,000,000 (more than the original 2,500 rows) but only 32 files in the table. While this table had many more rows, Spark only had 32 files to query, prunning almost all the files based on the **time** column, avoiding the small file problem we encountered in the partitioned table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "38f718c6-37fc-42ee-9b51-56367587000c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Demo Summary\n",
    "In the **iot_data** table, we did not partition the table when saving it. In this example, we allowed Spark to handle the saving process. Even though the dataset was much larger than the partitioned table from the first example, Spark optimized how the data was saved. It created 32 files for the Delta table, with each file containing a balanced number of rows, thus avoiding the \"small file\" problem that occurred with the partitioned table in the earlier example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cdc0b76d-bd16-4d95-9d0e-c14557131875",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "&copy; 2025 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the \n",
    "<a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/><a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | \n",
    "<a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | \n",
    "<a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {},
   "notebookName": "PO 1.1 - File Explosion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}